{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier, GradientBoostingRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, ElasticNet, Lasso, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON\n",
    "with open('algoparams_from_ui.json.rtf', 'r') as f: \n",
    "    rtfText = f.read() \n",
    "plainText = rtf_to_text(rtfText)\n",
    "mainDict = json.loads(plainText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Dicts in mainDict\n",
    "session_info = mainDict['design_state_data']['session_info']\n",
    "target = mainDict['design_state_data']['target']\n",
    "feature_handling = mainDict['design_state_data']['feature_handling']\n",
    "feature_reduction = mainDict['design_state_data']['feature_reduction']\n",
    "hyperparameters = mainDict['design_state_data']['hyperparameters']\n",
    "algorithms = mainDict['design_state_data']['algorithms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying algorithms to test\n",
    "algorithms['RandomForestRegressor']['is_selected'] = True\n",
    "algorithms['RandomForestClassifier']['is_selected'] = True\n",
    "algorithms['GBTClassifier']['is_selected'] = True\n",
    "algorithms['GBTRegressor']['is_selected'] = True\n",
    "algorithms['LinearRegression']['is_selected'] = True\n",
    "algorithms['LogisticRegression']['is_selected'] = True\n",
    "algorithms['RidgeRegression']['is_selected'] = True\n",
    "algorithms['LassoRegression']['is_selected'] = True\n",
    "algorithms['ElasticNetRegression']['is_selected'] = True\n",
    "algorithms['xg_boost']['is_selected'] = True\n",
    "algorithms['DecisionTreeRegressor']['is_selected'] = True\n",
    "algorithms['DecisionTreeClassifier']['is_selected'] = True\n",
    "algorithms['SVM']['is_selected'] = True\n",
    "algorithms['SGD']['is_selected'] = True\n",
    "algorithms['KNN']['is_selected'] = True\n",
    "algorithms['extra_random_trees']['is_selected'] = True\n",
    "algorithms['neural_network']['is_selected'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the target\n",
    "def getTarget(targetDict):\n",
    "    target = targetDict['target']\n",
    "    regtype = targetDict['type']\n",
    "    return target, regtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for GBT\n",
    "def lossGBT(algoDict):\n",
    "    if algoDict['GBTClassifier']['use_deviance'] == True:\n",
    "        return 'deviance'\n",
    "    elif algoDict['GBTClassifier']['use_exponential'] == True:\n",
    "        return 'exponential'\n",
    "    else:\n",
    "        return 'exponential'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for DT\n",
    "def critDT(algoDict, typ):\n",
    "    if typ == 'clf':\n",
    "        if algoDict['DecisionTreeClassifier']['use_gini'] == True:\n",
    "            return 'gini'\n",
    "        elif algoDict['DecisionTreeClassifier']['use_entropy'] == True:\n",
    "            return 'entropy'\n",
    "        else:\n",
    "            return 'gini'\n",
    "    elif typ == 'reg':\n",
    "        if algoDict['DecisionTreeRegressor']['use_gini'] == True:\n",
    "            return 'gini'\n",
    "        elif algoDict['DecisionTreeRegressor']['use_entropy'] == True:\n",
    "            return 'entropy'\n",
    "        else:\n",
    "            return 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for DT\n",
    "def splitDT(algoDict, typ):\n",
    "    if typ == 'clf':\n",
    "        if algoDict['DecisionTreeClassifier']['use_best'] == True:\n",
    "            return 'best'\n",
    "        elif algoDict['DecisionTreeClassifier']['use_random'] == True:\n",
    "            return 'random'\n",
    "        else:\n",
    "            return 'best'\n",
    "    elif typ == 'reg':\n",
    "        if algoDict['DecisionTreeRegressor']['use_best'] == True:\n",
    "            return 'best'\n",
    "        elif algoDict['DecisionTreeRegressor']['use_random'] == True:\n",
    "            return 'random'\n",
    "        else:\n",
    "            return 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for SVM\n",
    "def kernelSVM(algoDict):\n",
    "    if algoDict['SVM']['linear_kernel'] == True:\n",
    "        return 'linear'\n",
    "    elif algoDict['SVM']['polynomial_kernel'] == True:\n",
    "        return 'poly'\n",
    "    elif algoDict['SVM']['sigmoid_kernel'] == True:\n",
    "        return 'sigmoid'\n",
    "    else:\n",
    "        return 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for SVM\n",
    "def gammaSVM(algoDict):\n",
    "    if algoDict['SVM']['auto'] == True:\n",
    "        return 'auto'\n",
    "    elif algoDict['SVM']['scale'] == True:\n",
    "        return 'scale'\n",
    "    else:\n",
    "        return 'scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for SGD\n",
    "def lossSGD(algoDict):\n",
    "    if algoDict['SGD']['use_logistics'] == True:\n",
    "        return 'log'\n",
    "    elif algoDict['SGD']['use_modified_huber_loss'] == True:\n",
    "        return 'modified_huber'\n",
    "    else:\n",
    "        return 'hinge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for SGD\n",
    "def penaltySGD(algoDict):\n",
    "    if algoDict['SGD']['use_elastic_net_regularization'] == True:\n",
    "        return 'elasticnet'\n",
    "    elif algoDict['SGD']['use_l1_regularization'] == 'on':\n",
    "        return 'l1'\n",
    "    else:\n",
    "        return 'l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for kNN\n",
    "def weightKNN(algoDict):\n",
    "    if algoDict['KNN']['distance_weighting'] == True:\n",
    "        return 'distance'\n",
    "    else:\n",
    "        return 'uniform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in modelStr to return ScikitLearn object\n",
    "def transformStrToModelObjParams(modelStr, algoDict, regtype):\n",
    "    if modelStr == 'RandomForestClassifier':\n",
    "        return RandomForestClassifier(), {\n",
    "            'n_estimators' : [algoDict[modelStr]['max_trees']],\n",
    "            'max_depth' : [algoDict[modelStr]['max_depth']],\n",
    "            'min_samples_split' : [algoDict[modelStr]['min_samples_per_leaf_min_value']]\n",
    "        }\n",
    "\n",
    "    elif modelStr == 'RandomForestRegressor':\n",
    "        return RandomForestRegressor(), {\n",
    "            'n_estimators' : [algoDict[modelStr]['max_trees']],\n",
    "            'max_depth' : [algoDict[modelStr]['max_depth']],\n",
    "            'min_samples_split' : [algoDict[modelStr]['min_samples_per_leaf_min_value']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'GBTClassifier':\n",
    "        return GradientBoostingClassifier(\n",
    "            loss = lossGBT(algoDict)\n",
    "        ), {\n",
    "            'n_estimators' : algoDict[modelStr]['num_of_BoostingStages'],\n",
    "            'max_depth' : [algoDict[modelStr]['min_depth'],algoDict[modelStr]['max_depth']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'GBTRegressor':\n",
    "        return GradientBoostingRegressor(), {\n",
    "            'n_estimators' : algoDict[modelStr]['num_of_BoostingStages'],\n",
    "            'max_depth' : [algoDict[modelStr]['min_depth'],algoDict[modelStr]['max_depth']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'LinearRegression':\n",
    "        return LinearRegression(),{}\n",
    "    \n",
    "    elif modelStr == 'LogisticRegression':\n",
    "        return LogisticRegression(\n",
    "            penalty='elasticnet'\n",
    "        ), {\n",
    "            'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']],\n",
    "            'l1_ratio' : [algoDict[modelStr]['min_elasticnet'], algoDict[modelStr]['max_elasticnet']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'RidgeRegression':\n",
    "        if type(algoDict[modelStr]['regularization_term']) in [int, float]:\n",
    "            return Ridge(\n",
    "                alpha = algoDict[modelStr]['regularization_term']\n",
    "            ), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        else:\n",
    "            return Ridge(), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'LassoRegression':\n",
    "        if type(algoDict[modelStr]['regularization_term']) in [int, float]:\n",
    "            return Lasso(\n",
    "                alpha = algoDict[modelStr]['regularization_term']\n",
    "            ), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        else:\n",
    "            return Lasso(), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'ElasticNetRegression':\n",
    "        if type(algoDict[modelStr]['regularization_term']) in [int, float]:\n",
    "            return ElasticNet(\n",
    "                alpha = algoDict[modelStr]['regularization_term']\n",
    "            ), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        else:\n",
    "            return ElasticNet(), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'xg_boost':\n",
    "        if regtype == 'regression':\n",
    "            if algoDict[modelStr]['dart'] == True:\n",
    "                return XGBRegressor(\n",
    "                    booster = 'dart',\n",
    "                    random_state = algoDict[modelStr]['random_state']\n",
    "                ), {\n",
    "                    'max_depth' : algoDict[modelStr]['max_depth_of_tree'],\n",
    "                    'learning_rate' : algoDict[modelStr]['learningRate'],\n",
    "                    'gamma' : algoDict[modelStr]['gamma'],\n",
    "                    'min_child_weight' : algoDict[modelStr]['min_child_weight'],\n",
    "                    'reg_alpha' : algoDict[modelStr]['l1_regularization'],\n",
    "                    'reg_lambda' : algoDict[modelStr]['l2_regularization']\n",
    "                }\n",
    "            else:\n",
    "                return XGBRegressor(\n",
    "                    random_state = algoDict[modelStr]['random_state']\n",
    "                ), {\n",
    "                    'max_depth' : algoDict[modelStr]['max_depth_of_tree'],\n",
    "                    'learning_rate' : algoDict[modelStr]['learningRate'],\n",
    "                    'gamma' : algoDict[modelStr]['gamma'],\n",
    "                    'min_child_weight' : algoDict[modelStr]['min_child_weight'],\n",
    "                    'reg_alpha' : algoDict[modelStr]['l1_regularization'],\n",
    "                    'reg_lambda' : algoDict[modelStr]['l2_regularization']\n",
    "                }\n",
    "        elif regtype == 'classification':\n",
    "            if algoDict[modelStr]['dart'] == True:\n",
    "                return XGBClassifier(\n",
    "                    booster = 'dart',\n",
    "                    random_state = algoDict[modelStr]['random_state']\n",
    "                ), {\n",
    "                    'max_depth' : algoDict[modelStr]['max_depth_of_tree'],\n",
    "                    'learning_rate' : algoDict[modelStr]['learningRate'],\n",
    "                    'gamma' : algoDict[modelStr]['gamma'],\n",
    "                    'min_child_weight' : algoDict[modelStr]['min_child_weight'],\n",
    "                    'reg_alpha' : algoDict[modelStr]['l1_regularization'],\n",
    "                    'reg_lambda' : algoDict[modelStr]['l2_regularization']\n",
    "                }\n",
    "            else:\n",
    "                return XGBClassifier(\n",
    "                    random_state = algoDict[modelStr]['random_state']\n",
    "                ), {\n",
    "                    'max_depth' : algoDict[modelStr]['max_depth_of_tree'],\n",
    "                    'learning_rate' : algoDict[modelStr]['learningRate'],\n",
    "                    'gamma' : algoDict[modelStr]['gamma'],\n",
    "                    'min_child_weight' : algoDict[modelStr]['min_child_weight'],\n",
    "                    'reg_alpha' : algoDict[modelStr]['l1_regularization'],\n",
    "                    'reg_lambda' : algoDict[modelStr]['l2_regularization']\n",
    "                }\n",
    "            \n",
    "    elif modelStr == 'DecisionTreeClassifier':\n",
    "        return DecisionTreeClassifier(\n",
    "            criterion = critDT(algoDict, 'clf'),\n",
    "            splitter = splitDT(algoDict, 'clf')\n",
    "        ), {\n",
    "            'max_depth' : [algoDict[modelStr]['min_depth'], algoDict[modelStr]['max_depth']],\n",
    "            'min_samples_leaf' : algoDict[modelStr]['min_samples_per_leaf']\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'DecisionTreeRegressor':\n",
    "        return DecisionTreeRegressor(\n",
    "            splitter = splitDT(algoDict, 'reg')\n",
    "        ), {\n",
    "            'max_depth' : [algoDict[modelStr]['min_depth'], algoDict[modelStr]['max_depth']],\n",
    "            'min_samples_leaf' : algoDict[modelStr]['min_samples_per_leaf']\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'SVM':\n",
    "        if regtype == 'regression':\n",
    "            return SVR(\n",
    "                kernel = kernelSVM(algoDict),\n",
    "                gamma = gammaSVM(algoDict)\n",
    "            ), {\n",
    "                'C' : algoDict[modelStr]['c_value'],\n",
    "                'tol' : [algoDict[modelStr]['tolerance']],\n",
    "                'max_iter' : [algoDict[modelStr]['max_iterations']]\n",
    "            }\n",
    "        elif regtype == 'classification':\n",
    "            return SVC(\n",
    "                kernel = kernelSVM(algoDict),\n",
    "                gamma = gammaSVM(algoDict)\n",
    "            ), {\n",
    "                'C' : algoDict[modelStr]['c_value'],\n",
    "                'tol' : [algoDict[modelStr]['tolerance']],\n",
    "                'max_iter' : [algoDict[modelStr]['max_iterations']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'SGD':\n",
    "        return SGDClassifier(\n",
    "            loss = lossSGD(algoDict),\n",
    "            penalty = penaltySGD(algoDict)\n",
    "\n",
    "        ), {\n",
    "            'alpha' : algoDict[modelStr]['alpha_value'],\n",
    "            'tol' : [algoDict[modelStr]['tolerance']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'KNN':\n",
    "        if regtype == 'regression':\n",
    "            return KNeighborsRegressor(\n",
    "                weights = weightKNN(algoDict)\n",
    "            ), {\n",
    "                'n_neighbors' : algoDict[modelStr]['k_value']\n",
    "            }\n",
    "        elif regtype == 'classification':\n",
    "            return KNeighborsClassifier(\n",
    "                weights = weightKNN(algoDict)\n",
    "            ), {\n",
    "                'n_neighbors' : algoDict[modelStr]['k_value']\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'extra_random_trees':\n",
    "        if regtype == 'regression':\n",
    "            return ExtraTreesRegressor(), {\n",
    "                'n_estimators' : algoDict[modelStr]['num_of_trees'],\n",
    "                'max_depth' : algoDict[modelStr]['max_depth'],\n",
    "                'min_samples_leaf' : algoDict[modelStr]['min_samples_per_leaf']\n",
    "            }\n",
    "        elif regtype == 'classification':\n",
    "            return ExtraTreesClassifier(), {\n",
    "                'n_estimators' : algoDict[modelStr]['num_of_trees'],\n",
    "                'max_depth' : algoDict[modelStr]['max_depth'],\n",
    "                'min_samples_leaf' : algoDict[modelStr]['min_samples_per_leaf']\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'neural_network':\n",
    "        if regtype == 'regression':\n",
    "            return MLPRegressor(\n",
    "                alpha = algoDict[modelStr]['alpha_value'],\n",
    "                tol = algoDict[modelStr]['convergence_tolerance'],\n",
    "                early_stopping = algoDict[modelStr]['early_stopping'],\n",
    "                shuffle = algoDict[modelStr]['shuffle_data'],\n",
    "            ), {\n",
    "                'hidden_layer_sizes' : algoDict[modelStr]['hidden_layer_sizes']\n",
    "            }\n",
    "        elif regtype == 'classification':\n",
    "            return MLPClassifier(\n",
    "                alpha = algoDict[modelStr]['alpha_value'],\n",
    "                tol = algoDict[modelStr]['convergence_tolerance'],\n",
    "                early_stopping = algoDict[modelStr]['early_stopping'],\n",
    "                shuffle = algoDict[modelStr]['shuffle_data'],\n",
    "            ), {\n",
    "                'hidden_layer_sizes' : algoDict[modelStr]['hidden_layer_sizes']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use getTarget() and transformStrToModelObj() to return model object list\n",
    "def getAlgorithm(algoDict, targetDict):\n",
    "    _, regtype = getTarget(targetDict)\n",
    "    if regtype.lower() == 'regression':\n",
    "        possibleModels = ['RandomForestRegressor', 'GBTRegressor', 'LinearRegression',\n",
    "                          'RidgeRegression', 'LassoRegression', 'ElasticNetRegression',\n",
    "                          'xg_boost', 'DecisionTreeRegressor', 'neural_network',\n",
    "                          'SVM', 'KNN', 'extra_random_trees']\n",
    "        selectedModels = []\n",
    "        for models in possibleModels:\n",
    "            if algoDict[models]['is_selected'] == True:\n",
    "                selectedModels.append(models)\n",
    "        modelsObjDict = {}\n",
    "        modelsParamDict = {}\n",
    "        for models in selectedModels:\n",
    "            modelsObjDict[models], modelsParamDict[models] = transformStrToModelObjParams(models, algorithms, regtype)\n",
    "        return modelsObjDict, modelsParamDict\n",
    "        \n",
    "\n",
    "    elif regtype.lower() == 'classification':\n",
    "        possibleModels = ['RandomForestClassifier', 'GBTClassifier', 'LogisticRegression',\n",
    "                          'xg_boost', 'DecisionTreeClassifier', 'neural_network',\n",
    "                          'SVM', 'SGD', 'KNN', 'extra_random_trees']\n",
    "        selectedModels = []\n",
    "        for models in possibleModels:\n",
    "            if algoDict[models]['is_selected'] == True:\n",
    "                selectedModels.append(models)\n",
    "        modelsObjDict = {}\n",
    "        modelsParamDict = {}\n",
    "        for models in selectedModels:\n",
    "            modelsObjDict[models], modelsParamDict[models] = transformStrToModelObjParams(models, algorithms, regtype)\n",
    "        return modelsObjDict, modelsParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Handling\n",
    "def featureHandling(df):\n",
    "    for var, feature in feature_handling.items():\n",
    "        if feature['is_selected'] == False:\n",
    "            df.drop(var, axis=1, inplace=True)\n",
    "        if feature['feature_variable_type'] == 'numerical':\n",
    "            if feature['feature_details']['missing_values'] == 'Impute':\n",
    "                if feature['feature_details']['impute_with'] == 'Average of values':\n",
    "                    df[var].fillna(df[var].mean(), inplace=True)\n",
    "                elif feature['feature_details']['impute_with'] == 'custom':\n",
    "                    df[var].fillna(feature['feature_details']['impute_value'], inplace=True)\n",
    "        elif feature['feature_variable_type'] == 'text':\n",
    "            df[var] = LabelEncoder().fit_transform(df[var])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Reduction\n",
    "def featureReduction(df):\n",
    "    targetVar, regtype = getTarget(target)\n",
    "\n",
    "    X = df.drop(targetVar, axis=1).to_numpy()\n",
    "    y = df[targetVar].to_numpy()\n",
    "\n",
    "    if feature_reduction['feature_reduction_method'] == 'Tree-based':\n",
    "        if regtype == 'regression':\n",
    "            selector = SelectFromModel(\n",
    "                RandomForestRegressor(\n",
    "                n_estimators=int(feature_reduction['num_of_trees']),\n",
    "                max_depth=int(feature_reduction['depth_of_trees'])\n",
    "                )).fit(X, y)\n",
    "\n",
    "        elif regtype == 'classification':\n",
    "            selector = SelectFromModel(RandomForestClassifier(\n",
    "                n_estimators=int(feature_reduction['num_of_trees']),\n",
    "                max_depth=int(feature_reduction['depth_of_trees'])\n",
    "                )).fit(X, y)\n",
    "            \n",
    "        featImp = selector.estimator_.feature_importances_\n",
    "        featImp = [(j,i) for i,j in enumerate(featImp)]\n",
    "        featImp.sort(reverse=True)\n",
    "        featImpIndex = [k for _, k in featImp[:eval(feature_reduction['num_of_features_to_keep'])]]\n",
    "        \n",
    "        tempdf = df.iloc[:,featImpIndex]\n",
    "        return tempdf.to_numpy(), y\n",
    "\n",
    "    elif feature_reduction['feature_reduction_method'] == 'Principal Component Analysis':\n",
    "        pca = PCA(n_components=int(feature_reduction['num_of_features_to_keep'])).fit(X)\n",
    "        X = pca.transform(X)\n",
    "        return X, y\n",
    "\n",
    "    elif feature_reduction['feature_reduction_method'] == 'Correlation with target':\n",
    "        corr = df.corr()[targetVar].drop(targetVar)\n",
    "        corr = [(j,i) for i,j in zip(corr.index, corr.values)]\n",
    "        corr.sort(reverse=True)\n",
    "        corrIndex = [k for _, k in corr[:eval(feature_reduction['num_of_features_to_keep'])]]\n",
    "        \n",
    "        tempdf = df.loc[:,corrIndex]\n",
    "        return tempdf.to_numpy(), y\n",
    "\n",
    "    elif feature_reduction['feature_reduction_method'] == 'No Reduction':\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = session_info['dataset']\n",
    "df = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = featureHandling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = featureReduction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsObjScikitDict, paramDict = getAlgorithm(algorithms, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor\n",
      "Best parameters: {'max_depth': 25, 'min_samples_split': 5, 'n_estimators': 20}\n",
      "Best score: 0.9839125935269852\n",
      "RandomForestRegressor(max_depth=25, min_samples_split=5, n_estimators=20)\n",
      "\n",
      "\n",
      "GBTRegressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 7, 'n_estimators': 89}\n",
      "Best score: 0.9806487575644894\n",
      "GradientBoostingRegressor(max_depth=7, n_estimators=89)\n",
      "\n",
      "\n",
      "LinearRegression\n",
      "Best parameters: {}\n",
      "Best score: 1.0\n",
      "LinearRegression()\n",
      "\n",
      "\n",
      "RidgeRegression\n",
      "Best parameters: {'max_iter': 30}\n",
      "Best score: 0.9740893660573825\n",
      "Ridge(max_iter=30)\n",
      "\n",
      "\n",
      "LassoRegression\n",
      "Best parameters: {'max_iter': 30}\n",
      "Best score: -31.363842543455746\n",
      "Lasso(max_iter=30)\n",
      "\n",
      "\n",
      "ElasticNetRegression\n",
      "Best parameters: {'max_iter': 30}\n",
      "Best score: -11.939405947092737\n",
      "ElasticNet(max_iter=30)\n",
      "\n",
      "\n",
      "xg_boost\n",
      "Best parameters: {'gamma': 68, 'learning_rate': 89, 'max_depth': 56, 'min_child_weight': 67, 'reg_alpha': 77, 'reg_lambda': 78}\n",
      "Best score: -32.586369171789656\n",
      "XGBRegressor(base_score=None, booster='dart', callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=68, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=89, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=56, max_leaves=None,\n",
      "             min_child_weight=67, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=0, ...)\n",
      "\n",
      "\n",
      "DecisionTreeRegressor\n",
      "Best parameters: {'max_depth': 7, 'min_samples_leaf': 6}\n",
      "Best score: 0.7282600979451564\n",
      "DecisionTreeRegressor(max_depth=7, min_samples_leaf=6)\n",
      "\n",
      "\n",
      "neural_network\n",
      "Best parameters: {'hidden_layer_sizes': 89}\n",
      "Best score: -0.15653246284211733\n",
      "MLPRegressor(alpha=0, early_stopping=True, hidden_layer_sizes=89, tol=0)\n",
      "\n",
      "\n",
      "SVM\n",
      "Best parameters: {'C': 566, 'max_iter': 7, 'tol': 7}\n",
      "Best score: -24.479567031003292\n",
      "SVR(C=566, gamma='auto', kernel='linear', max_iter=7, tol=7)\n",
      "\n",
      "\n",
      "KNN\n",
      "Best parameters: {'n_neighbors': 78}\n",
      "Best score: -2.515505639021459\n",
      "KNeighborsRegressor(n_neighbors=78, weights='distance')\n",
      "\n",
      "\n",
      "extra_random_trees\n",
      "Best parameters: {'max_depth': 45, 'min_samples_leaf': 56, 'n_estimators': 45}\n",
      "Best score: -28.36075182776263\n",
      "ExtraTreesRegressor(max_depth=45, min_samples_leaf=56, n_estimators=45)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in paramDict.keys():\n",
    "    print(model)\n",
    "    finalModel = GridSearchCV(modelsObjScikitDict[model], paramDict[model], cv=5, n_jobs=-1)\n",
    "    finalModel.fit(X, y)\n",
    "    print(f\"Best parameters: {finalModel.best_params_}\")\n",
    "    print(f\"Best score: {finalModel.best_score_}\")\n",
    "    print(finalModel.best_estimator_)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
