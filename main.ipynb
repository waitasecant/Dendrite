{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier, GradientBoostingRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, ElasticNet, Lasso, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer, f1_score\n",
    "# from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON\n",
    "with open('algoparams_from_ui.json.rtf', 'r') as f: \n",
    "    rtfText = f.read() \n",
    "plainText = rtf_to_text(rtfText)\n",
    "mainDict = json.loads(plainText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Dicts in mainDict\n",
    "session_info = mainDict['design_state_data']['session_info']\n",
    "target = mainDict['design_state_data']['target']\n",
    "# train = mainDict['design_state_data']['train']\n",
    "# metrics = mainDict['design_state_data']['metrics']\n",
    "feature_handling = mainDict['design_state_data']['feature_handling']\n",
    "# feature_generation = mainDict['design_state_data']['feature_generation']\n",
    "feature_reduction = mainDict['design_state_data']['feature_reduction']\n",
    "hyperparameters = mainDict['design_state_data']['hyperparameters']\n",
    "# weighting_stratergy = mainDict['design_state_data']['weighting_stratergy']\n",
    "# probability_calibration = mainDict['design_state_data']['probability_calibration']\n",
    "algorithms = mainDict['design_state_data']['algorithms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying algorithms to test\n",
    "algorithms['RandomForestClassifier']['is_selected'] = True\n",
    "algorithms['GBTRegressor']['is_selected'] = True\n",
    "algorithms['GBTRegressor']['use_deviance'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the target\n",
    "def getTarget(targetDict):\n",
    "    target = targetDict['target']\n",
    "    regtype = targetDict['type']\n",
    "    return target, regtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for GBT\n",
    "def lossGBT(algoDict, typ):\n",
    "    if typ == 'clf':\n",
    "        if algoDict['GBTClassifier']['use_deviance'] == True:\n",
    "            return 'deviance'\n",
    "        elif algoDict['GBTClassifier']['use_exponential'] == True:\n",
    "            return 'exponential'\n",
    "        else:\n",
    "            return 'exponential'\n",
    "    elif typ == 'reg':\n",
    "        if algoDict['GBTRegressor']['use_deviance'] == True:\n",
    "            return 'deviance'\n",
    "        elif algoDict['GBTRegressor']['use_exponential'] == True:\n",
    "            return 'exponential'\n",
    "        else:\n",
    "            return 'exponential' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for DT\n",
    "def critDT(algoDict, typ):\n",
    "    if typ == 'clf':\n",
    "        if algoDict['DecisionTreeClassifier']['use_gini'] == True:\n",
    "            return 'gini'\n",
    "        elif algoDict['DecisionTreeClassifier']['use_entropy'] == True:\n",
    "            return 'entropy'\n",
    "        else:\n",
    "            return 'gini'\n",
    "    elif typ == 'reg':\n",
    "        if algoDict['DecisionTreeRegressor']['use_gini'] == True:\n",
    "            return 'gini'\n",
    "        elif algoDict['DecisionTreeRegressor']['use_entropy'] == True:\n",
    "            return 'entropy'\n",
    "        else:\n",
    "            return 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for DT\n",
    "def splitDT(algoDict, typ):\n",
    "    if typ == 'clf':\n",
    "        if algoDict['DecisionTreeClassifier']['use_best'] == True:\n",
    "            return 'best'\n",
    "        elif algoDict['DecisionTreeClassifier']['use_random'] == True:\n",
    "            return 'random'\n",
    "        else:\n",
    "            return 'best'\n",
    "    elif typ == 'reg':\n",
    "        if algoDict['DecisionTreeRegressor']['use_best'] == True:\n",
    "            return 'best'\n",
    "        elif algoDict['DecisionTreeRegressor']['use_random'] == True:\n",
    "            return 'random'\n",
    "        else:\n",
    "            return 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for SVM\n",
    "def kernelSVM(algoDict):\n",
    "    if algoDict['SVM']['linear_kernel'] == True:\n",
    "        return 'linear'\n",
    "    elif algoDict['SVM']['polynomial_kernel'] == True:\n",
    "        return 'poly'\n",
    "    elif algoDict['SVM']['sigmoid_kernel'] == True:\n",
    "        return 'sigmoid'\n",
    "    else:\n",
    "        return 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for SVM\n",
    "def gammaSVM(algoDict):\n",
    "    if algoDict['SVM']['auto'] == True:\n",
    "        return 'auto'\n",
    "    elif algoDict['SVM']['scale'] == True:\n",
    "        return 'scale'\n",
    "    else:\n",
    "        return 'scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for SGD\n",
    "def lossSGD(algoDict):\n",
    "    if algoDict['SGD']['use_logistics'] == True:\n",
    "        return 'log'\n",
    "    elif algoDict['SGD']['use_modified_huber_loss'] == True:\n",
    "        return 'modified_huber'\n",
    "    else:\n",
    "        return 'hinge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for SGD\n",
    "def penaltySGD(algoDict):\n",
    "    if algoDict['SGD']['use_elastic_net_regularization'] == True:\n",
    "        return 'elasticnet'\n",
    "    elif algoDict['SGD']['use_l1_regularization'] == 'on':\n",
    "        return 'l1'\n",
    "    else:\n",
    "        return 'l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for kNN\n",
    "def weightKNN(algoDict):\n",
    "    if algoDict['KNN']['distance_weighting'] == True:\n",
    "        return 'distance'\n",
    "    else:\n",
    "        return 'uniform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in modelStr to return ScikitLearn object\n",
    "def transformStrToModelObjParams(modelStr, algoDict, regtype):\n",
    "    if modelStr == 'RandomForestClassifier':\n",
    "        return RandomForestClassifier(), {\n",
    "            'n_estimators' : [algoDict[modelStr]['max_trees']],\n",
    "            'max_depth' : [algoDict[modelStr]['max_depth']],\n",
    "            'min_samples_split' : [algoDict[modelStr]['min_samples_per_leaf_min_value']]\n",
    "        }\n",
    "\n",
    "    elif modelStr == 'RandomForestRegressor':\n",
    "        return RandomForestRegressor(), {\n",
    "            'n_estimators' : [algoDict[modelStr]['max_trees']],\n",
    "            'max_depth' : [algoDict[modelStr]['max_depth']],\n",
    "            'min_samples_split' : [algoDict[modelStr]['min_samples_per_leaf_min_value']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'GBTClassifier':\n",
    "        return GradientBoostingClassifier(\n",
    "            loss = lossGBT(algoDict, 'clf')\n",
    "        ), {\n",
    "            'n_estimators' : algoDict[modelStr]['num_of_BoostingStages'],\n",
    "            'max_depth' : [algoDict[modelStr]['min_depth'],algoDict[modelStr]['max_depth']],\n",
    "            'subsample' : [algoDict[modelStr]['min_subsample'], algoDict[modelStr]['max_subsample']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'GBTRegressor':\n",
    "        return GradientBoostingRegressor(\n",
    "            loss = lossGBT(algoDict, 'clf')\n",
    "        ), {\n",
    "            'n_estimators' : algoDict[modelStr]['num_of_BoostingStages'],\n",
    "            'max_depth' : [algoDict[modelStr]['min_depth'],algoDict[modelStr]['max_depth']],\n",
    "            'subsample' : [algoDict[modelStr]['min_subsample'], algoDict[modelStr]['max_subsample']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'LinearRegression':\n",
    "        return LinearRegression(),{}\n",
    "    \n",
    "    elif modelStr == 'LogisticRegression':\n",
    "        return LogisticRegression(\n",
    "            penalty='elasticnet'\n",
    "        ), {\n",
    "            'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']],\n",
    "            'l1_ratio' : [algoDict[modelStr]['min_elasticnet'], algoDict[modelStr]['max_elasticnet']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'RidgeRegression':\n",
    "        if type(algoDict[modelStr]['regularization_term']) in [int, float]:\n",
    "            return Ridge(\n",
    "                alpha = algoDict[modelStr]['regularization_term']\n",
    "            ), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        else:\n",
    "            return Ridge(), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'LassoRegression':\n",
    "        if type(algoDict[modelStr]['regularization_term']) in [int, float]:\n",
    "            return Lasso(\n",
    "                alpha = algoDict[modelStr]['regularization_term']\n",
    "            ), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        else:\n",
    "            return Lasso(), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'ElasticNetRegression':\n",
    "        if type(algoDict[modelStr]['regularization_term']) in [int, float]:\n",
    "            return ElasticNet(\n",
    "                alpha = algoDict[modelStr]['regularization_term']\n",
    "            ), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        else:\n",
    "            return ElasticNet(), {\n",
    "                'max_iter' : [algoDict[modelStr]['min_iter'], algoDict[modelStr]['max_iter']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'xgboost':\n",
    "        if regtype == 'regression':\n",
    "            if algoDict[modelStr]['dart'] == True:\n",
    "                return XGBRegressor(\n",
    "                    booster = 'dart',\n",
    "                    random_state = algoDict[modelStr]['random_state']\n",
    "                ), {\n",
    "                    'early_stopping_rounds' : [algoDict[modelStr]['early_stopping_rounds']],\n",
    "                    'max_depth' : algoDict[modelStr]['max_depth_of_tree'],\n",
    "                    'learning_rate' : algoDict[modelStr]['learningRate'],\n",
    "                    'gamma' : algoDict[modelStr]['gamma'],\n",
    "                    'min_child_weight' : algoDict[modelStr]['min_child_weight'],\n",
    "                    'reg_alpha' : algoDict[modelStr]['l1_regularization'],\n",
    "                    'reg_lambda' : algoDict[modelStr]['l2_regularization'],\n",
    "                    'colsample_bytree' : algoDict[modelStr]['col_sample_by_tree'],\n",
    "                    'subsample' : algoDict[modelStr]['sub_sample'],\n",
    "                }\n",
    "            else:\n",
    "                return XGBRegressor(\n",
    "                    random_state = algoDict[modelStr]['random_state']\n",
    "                ), {\n",
    "                    'early_stopping_rounds' : [algoDict[modelStr]['early_stopping_rounds']],\n",
    "                    'max_depth' : algoDict[modelStr]['max_depth_of_tree'],\n",
    "                    'learning_rate' : algoDict[modelStr]['learningRate'],\n",
    "                    'gamma' : algoDict[modelStr]['gamma'],\n",
    "                    'min_child_weight' : algoDict[modelStr]['min_child_weight'],\n",
    "                    'reg_alpha' : algoDict[modelStr]['l1_regularization'],\n",
    "                    'reg_lambda' : algoDict[modelStr]['l2_regularization'],\n",
    "                    'colsample_bytree' : algoDict[modelStr]['col_sample_by_tree'],\n",
    "                    'subsample' : algoDict[modelStr]['sub_sample'],\n",
    "                }\n",
    "        elif regtype == 'classification':\n",
    "            if algoDict[modelStr]['dart'] == True:\n",
    "                return XGBClassifier(\n",
    "                    booster = 'dart',\n",
    "                    random_state = algoDict[modelStr]['random_state']\n",
    "                ), {\n",
    "                    'early_stopping_rounds' : [algoDict[modelStr]['early_stopping_rounds']],\n",
    "                    'max_depth' : algoDict[modelStr]['max_depth_of_tree'],\n",
    "                    'learning_rate' : algoDict[modelStr]['learningRate'],\n",
    "                    'gamma' : algoDict[modelStr]['gamma'],\n",
    "                    'min_child_weight' : algoDict[modelStr]['min_child_weight'],\n",
    "                    'reg_alpha' : algoDict[modelStr]['l1_regularization'],\n",
    "                    'reg_lambda' : algoDict[modelStr]['l2_regularization'],\n",
    "                    'colsample_bytree' : algoDict[modelStr]['col_sample_by_tree'],\n",
    "                    'subsample' : algoDict[modelStr]['sub_sample'],\n",
    "                }\n",
    "            else:\n",
    "                return XGBClassifier(\n",
    "                    random_state = algoDict[modelStr]['random_state']\n",
    "                ), {\n",
    "                    'early_stopping_rounds' : [algoDict[modelStr]['early_stopping_rounds']],\n",
    "                    'max_depth' : algoDict[modelStr]['max_depth_of_tree'],\n",
    "                    'learning_rate' : algoDict[modelStr]['learningRate'],\n",
    "                    'gamma' : algoDict[modelStr]['gamma'],\n",
    "                    'min_child_weight' : algoDict[modelStr]['min_child_weight'],\n",
    "                    'reg_alpha' : algoDict[modelStr]['l1_regularization'],\n",
    "                    'reg_lambda' : algoDict[modelStr]['l2_regularization'],\n",
    "                    'colsample_bytree' : algoDict[modelStr]['col_sample_by_tree'],\n",
    "                    'subsample' : algoDict[modelStr]['sub_sample'],\n",
    "                }\n",
    "            \n",
    "    elif modelStr == 'DecisionTreeClassifier':\n",
    "        return DecisionTreeClassifier(\n",
    "            criterion = critDT(algoDict, 'clf'),\n",
    "            splitter = splitDT(algoDict, 'clf')\n",
    "        ), {\n",
    "            'max_depth' : [algoDict[modelStr]['min_depth'], algoDict[modelStr]['max_depth']],\n",
    "            'min_samples_leaf' : algoDict[modelStr]['min_samples_per_leaf']\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'DecisionTreeRegressor':\n",
    "        return DecisionTreeRegressor(\n",
    "            criterion = critDT(algoDict, 'clf'),\n",
    "            splitter = splitDT(algoDict, 'clf')\n",
    "        ), {\n",
    "            'max_depth' : [algoDict[modelStr]['min_depth'], algoDict[modelStr]['max_depth']],\n",
    "            'min_samples_leaf' : algoDict[modelStr]['min_samples_per_leaf']\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'SVM':\n",
    "        if regtype == 'regression':\n",
    "            return SVR(\n",
    "                kernel = kernelSVM(algoDict),\n",
    "                gamma = gammaSVM(algoDict)\n",
    "            ), {\n",
    "                'C' : algoDict[modelStr]['c_value'],\n",
    "                'tol' : [algoDict[modelStr]['tolerance']],\n",
    "                'max_iter' : [algoDict[modelStr]['max_iterations']]\n",
    "            }\n",
    "        elif regtype == 'classification':\n",
    "            return SVC(\n",
    "                kernel = kernelSVM(algoDict),\n",
    "                gamma = gammaSVM(algoDict)\n",
    "            ), {\n",
    "                'C' : algoDict[modelStr]['c_value'],\n",
    "                'tol' : [algoDict[modelStr]['tolerance']],\n",
    "                'max_iter' : [algoDict[modelStr]['max_iterations']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'SGD':\n",
    "        return SGDClassifier(\n",
    "            loss = lossSGD(algoDict),\n",
    "            penalty = penaltySGD(algoDict)\n",
    "\n",
    "        ), {\n",
    "            'alpha' : algoDict[modelStr]['alpha_value'],\n",
    "            'tol' : [algoDict[modelStr]['tolerance']]\n",
    "        }\n",
    "    \n",
    "    elif modelStr == 'KNN':\n",
    "        if regtype == 'regression':\n",
    "            return KNeighborsRegressor(\n",
    "                weights = weightKNN(algoDict)\n",
    "            ), {\n",
    "                'n_neighbors' : algoDict[modelStr]['k_value'],\n",
    "                'p' : [algoDict[modelStr]['p_value']]\n",
    "            }\n",
    "        elif regtype == 'classification':\n",
    "            return KNeighborsClassifier(\n",
    "                weights = weightKNN(algoDict)\n",
    "            ), {\n",
    "                'n_neighbors' : algoDict[modelStr]['k_value'],\n",
    "                'p' : [algoDict[modelStr]['p_value']]\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'extra_random_trees':\n",
    "        if regtype == 'regression':\n",
    "            return ExtraTreesRegressor(), {\n",
    "                'n_estimators' : algoDict[modelStr]['num_of_trees'],\n",
    "                'max_depth' : algoDict[modelStr]['max_depth'],\n",
    "                'min_samples_leaf' : algoDict[modelStr]['min_sample_per_leaf']\n",
    "            }\n",
    "        elif regtype == 'classification':\n",
    "            return ExtraTreesClassifier(), {\n",
    "                'n_estimators' : algoDict[modelStr]['num_of_trees'],\n",
    "                'max_depth' : algoDict[modelStr]['max_depth'],\n",
    "                'min_samples_leaf' : algoDict[modelStr]['min_sample_per_leaf']\n",
    "            }\n",
    "        \n",
    "    elif modelStr == 'neural_netwrok':\n",
    "        if regtype == 'regression':\n",
    "            return MLPRegressor(\n",
    "                alpha = algoDict[modelStr]['alpha_value'],\n",
    "                max_iter = algoDict[modelStr]['max_iterations'],\n",
    "                tol = algoDict[modelStr]['convergence_tolerance'],\n",
    "                early_stopping = algoDict[modelStr]['early_stopping'],\n",
    "                shuffle = algoDict[modelStr]['shuffle_data'],\n",
    "                learning_rate_init = algoDict[modelStr]['initial_learning_rate'],\n",
    "                beta_1 = algoDict[modelStr]['beta_1'],\n",
    "                beta_2 = algoDict[modelStr]['beta_2'],\n",
    "                epsilon = algoDict[modelStr]['epsilon'],\n",
    "                power_t = algoDict[modelStr]['power_t'],\n",
    "                momentum = algoDict[modelStr]['momentum'],\n",
    "            ), {\n",
    "                'hidden_layer_sizes' : algoDict[modelStr]['hidden_layer_sizes'],\n",
    "            }\n",
    "        elif regtype == 'classification':\n",
    "            return MLPClassifier(\n",
    "                alpha = algoDict[modelStr]['alpha_value'],\n",
    "                max_iter = algoDict[modelStr]['max_iterations'],\n",
    "                tol = algoDict[modelStr]['convergence_tolerance'],\n",
    "                early_stopping = algoDict[modelStr]['early_stopping'],\n",
    "                shuffle = algoDict[modelStr]['shuffle_data'],\n",
    "                learning_rate_init = algoDict[modelStr]['initial_learning_rate'],\n",
    "                beta_1 = algoDict[modelStr]['beta_1'],\n",
    "                beta_2 = algoDict[modelStr]['beta_2'],\n",
    "                epsilon = algoDict[modelStr]['epsilon'],\n",
    "                power_t = algoDict[modelStr]['power_t'],\n",
    "                momentum = algoDict[modelStr]['momentum'],\n",
    "            ), {\n",
    "                'hidden_layer_sizes' : algoDict[modelStr]['hidden_layer_sizes'],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use getTarget() and transformStrToModelObj() to return model object list\n",
    "def getAlgorithm(algoDict, targetDict):\n",
    "    _, regtype = getTarget(targetDict)\n",
    "    if regtype.lower() == 'regression':\n",
    "        possibleModels = ['RandomForestRegressor', 'GBTRegressor', 'LinearRegression',\n",
    "                          'RidgeRegression', 'LassoRegression', 'ElasticNetRegression',\n",
    "                          'xg_boost', 'DecisionTreeRegressor', 'neural_network',\n",
    "                          'SVM', 'KNN', 'extra_random_trees']\n",
    "        selectedModels = []\n",
    "        for models in possibleModels:\n",
    "            if algoDict[models]['is_selected'] == True:\n",
    "                selectedModels.append(models)\n",
    "        modelsObjDict = {}\n",
    "        modelsParamDict = {}\n",
    "        for models in selectedModels:\n",
    "            modelsObjDict[models], modelsParamDict[models] = transformStrToModelObjParams(models, algorithms, regtype)\n",
    "        return modelsObjDict, modelsParamDict\n",
    "        \n",
    "\n",
    "    elif regtype.lower() == 'classification':\n",
    "        possibleModels = ['RandomForestClassifier', 'GBTClassifier', 'LogisticRegression',\n",
    "                          'xg_boost', 'DecisionTreeClassifier', 'neural_network',\n",
    "                          'SVM', 'SGD', 'KNN', 'extra_random_trees']\n",
    "        selectedModels = []\n",
    "        for models in possibleModels:\n",
    "            if algoDict[models]['is_selected'] == True:\n",
    "                selectedModels.append(models)\n",
    "        modelsObjDict = {}\n",
    "        modelsParamDict = {}\n",
    "        for models in selectedModels:\n",
    "            modelsObjDict[models], modelsParamDict[models] = transformStrToModelObjParams(models, algorithms, regtype)\n",
    "        return modelsObjDict, modelsParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Handling\n",
    "def featureHandling(df):\n",
    "    for var, feature in feature_handling.items():\n",
    "        if feature['is_selected'] == False:\n",
    "            df.drop(var, axis=1, inplace=True)\n",
    "        if feature['feature_variable_type'] == 'numerical':\n",
    "            if feature['feature_details']['missing_values'] == 'Impute':\n",
    "                if feature['feature_details']['impute_with'] == 'Average of values':\n",
    "                    df[var].fillna(df[var].mean(), inplace=True)\n",
    "                elif feature['feature_details']['impute_with'] == 'custom':\n",
    "                    df[var].fillna(feature['feature_details']['impute_value'], inplace=True)\n",
    "        elif feature['feature_variable_type'] == 'text':\n",
    "            df[var] = LabelEncoder().fit_transform(df[var])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Reduction\n",
    "def featureReduction(df):\n",
    "    targetVar, regtype = getTarget(target)\n",
    "\n",
    "    X = df.drop(targetVar, axis=1).to_numpy()\n",
    "    y = df[targetVar].to_numpy()\n",
    "\n",
    "    if feature_reduction['feature_reduction_method'] == 'Tree-based':\n",
    "        if regtype == 'regression':\n",
    "            selector = SelectFromModel(\n",
    "                RandomForestRegressor(\n",
    "                n_estimators=int(feature_reduction['num_of_trees']),\n",
    "                max_depth=int(feature_reduction['depth_of_trees'])\n",
    "                )).fit(X, y)\n",
    "\n",
    "        elif regtype == 'classification':\n",
    "            selector = SelectFromModel(RandomForestClassifier(\n",
    "                n_estimators=int(feature_reduction['num_of_trees']),\n",
    "                max_depth=int(feature_reduction['depth_of_trees'])\n",
    "                )).fit(X, y)\n",
    "            \n",
    "        featImp = selector.estimator_.feature_importances_\n",
    "        featImp = [(j,i) for i,j in enumerate(featImp)]\n",
    "        featImp.sort(reverse=True)\n",
    "        featImpIndex = [y for _, y in featImp[:eval(feature_reduction['num_of_features_to_keep'])]]\n",
    "        \n",
    "        tempdf = df.iloc[:,featImpIndex]\n",
    "        return tempdf.to_numpy()\n",
    "\n",
    "    elif feature_reduction['feature_reduction_method'] == 'Principal Component Analysis':\n",
    "        pca = PCA(n_components=int(feature_reduction['num_of_features_to_keep'])).fit(X)\n",
    "        X = pca.transform(X)\n",
    "        return X\n",
    "\n",
    "    elif feature_reduction['feature_reduction_method'] == 'Correlation with target':\n",
    "        corr = df.corr()[targetVar].drop(targetVar)\n",
    "        corr = [(j,i) for i,j in zip(corr.index, corr.values)]\n",
    "        corr.sort(reverse=True)\n",
    "        corrIndex = [y for _, y in corr[:eval(feature_reduction['num_of_features_to_keep'])]]\n",
    "        \n",
    "        tempdf = df.loc[:,corrIndex]\n",
    "        return tempdf.to_numpy()\n",
    "\n",
    "    elif feature_reduction['feature_reduction_method'] == 'No Reduction':\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,p = getAlgorithm(algorithms, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForestRegressor': RandomForestRegressor(),\n",
       " 'GBTRegressor': GradientBoostingRegressor(loss='deviance')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForestRegressor': {'n_estimators': [20],\n",
       "  'max_depth': [25],\n",
       "  'min_samples_split': [5]},\n",
       " 'GBTRegressor': {'n_estimators': [67, 89],\n",
       "  'max_depth': [5, 7],\n",
       "  'subsample': [1, 2]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = session_info['dataset']\n",
    "df = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = featureHandling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = featureReduction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [hyperparameters[\"min_trees\"], hyperparameters[\"max_trees\"]],\n",
    "    'max_depth': [hyperparameters[\"min_depth\"], hyperparameters[\"max_depth\"]],\n",
    "    'min_samples_leaf': [hyperparameters[\"min_samples_per_leaf_min_value\"],\n",
    "                            hyperparameters[\"min_samples_per_leaf_max_value\"]]\n",
    "    }\n",
    "model = GridSearchCV(RandomForestRegressor(), parameters, cv=5, n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
